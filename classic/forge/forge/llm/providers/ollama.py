from __future__ import annotations

import time
import json
import re

import enum
import logging
from typing import (
    Any,
    Callable,
    Optional,
    Sequence,
    TypeVar,
)
from forge.json.parsing import extract_dict_from_json

import tiktoken
from pydantic import SecretStr
import httpx
from tenacity import retry, stop_after_attempt, wait_fixed
from pydantic import ValidationError

from forge.models.config import UserConfigurable

from ._openai_base import BaseOpenAIChatProvider
from forge.json.parsing import json_loads
from typing import TYPE_CHECKING

from .schema import (
    AssistantChatMessage,
    ChatMessage,
    ChatModelResponse,
    CompletionModelFunction,
    ChatModelInfo,
    ModelProviderBudget,
    ModelProviderConfiguration,
    ModelProviderName,
    ModelProviderCredentials,
    ModelProviderSettings,
    ModelTokenizer,
    BaseModel
)

from openai.types.chat import (
    ChatCompletionMessage,
    ChatCompletion,
    CompletionCreateParams,
)

from typing import (
    Any,
    Optional,
    Sequence,
)


_T = TypeVar("_T")
    
class OllamaModelName(str, enum.Enum):
    LLAMA3_1_8B = "llama3.1"
    LLAMA3_8B = "llama3"
    LLAMA3_70B = "llama3"
    GEMMA2 = "gemma2"
    PHI3_MINI = "phi3:mini"
    PHI3_MEDIUM = "phi3:medium"
    PHI3_MEDIUM_128K = "phi3:medium-128k"

OLLAMA_CHAT_MODELS = {
    info.name: info
    for info in [
        ChatModelInfo(
            name=OllamaModelName.LLAMA3_1_8B,
            provider_name=ModelProviderName.OLLAMA,
            prompt_token_cost=0.05 / 1e6,
            completion_token_cost=0.10 / 1e6,
            max_tokens=128000,
            has_function_call_api=True,
        ),
        ChatModelInfo(
            name=OllamaModelName.LLAMA3_8B,
            provider_name=ModelProviderName.OLLAMA,
            prompt_token_cost=0.05 / 1e6,
            completion_token_cost=0.10 / 1e6,
            max_tokens=8192,
            has_function_call_api=True,
        ),
        ChatModelInfo(
            name=OllamaModelName.LLAMA3_70B,
            provider_name=ModelProviderName.OLLAMA,
            prompt_token_cost=0.59 / 1e6,
            completion_token_cost=0.79 / 1e6,
            max_tokens=8192,
            has_function_call_api=True,
        ),
        ChatModelInfo(
            name=OllamaModelName.GEMMA2,
            provider_name=ModelProviderName.OLLAMA,
            prompt_token_cost=0.59 / 1e6,
            completion_token_cost=0.79 / 1e6,
            max_tokens=8192,
            has_function_call_api=True,
        ),
        ChatModelInfo(
            name=OllamaModelName.PHI3_MINI,
            provider_name=ModelProviderName.OLLAMA,
            prompt_token_cost=0.59 / 1e6,
            completion_token_cost=0.79 / 1e6,
            max_tokens=128000,
            has_function_call_api=True,
        ),
        ChatModelInfo(
            name=OllamaModelName.PHI3_MEDIUM,
            provider_name=ModelProviderName.OLLAMA,
            prompt_token_cost=0.59 / 1e6,
            completion_token_cost=0.79 / 1e6,
            max_tokens=128000,
            has_function_call_api=True,
        ),
        ChatModelInfo(
            name=OllamaModelName.PHI3_MEDIUM_128K,
            provider_name=ModelProviderName.OLLAMA,
            prompt_token_cost=0.59 / 1e6,
            completion_token_cost=0.79 / 1e6,
            max_tokens=128000,
            has_function_call_api=True,
        ),
    ]
}

class OllamaCredentials(ModelProviderCredentials):
    """Credentials for Ollama."""
    api_key: SecretStr = UserConfigurable(from_env="OLLAMA_API_KEY")  # type: ignore
    api_base: Optional[SecretStr] = UserConfigurable(
        default=None, from_env="OLLAMA_API_BASE_URL"
    )

    def get_api_access_kwargs(self) -> dict[str, str]:
        return {
            k: v.get_secret_value()
            for k, v in {
                "api_key": self.api_key,
                "base_url": self.api_base,
            }.items()
            if v is not None
        }

from typing_extensions import Literal


class Choice(BaseModel):
    finish_reason: Literal["stop", "length", "tool_calls", "content_filter", "function_call"]
    """The reason the model stopped generating tokens.

    This will be `stop` if the model hit a natural stop point or a provided stop
    sequence, `length` if the maximum number of tokens specified in the request was
    reached, `content_filter` if content was omitted due to a flag from our content
    filters, `tool_calls` if the model called a tool, or `function_call`
    (deprecated) if the model called a function.
    """

    index: int
    """The index of the choice in the list of choices."""

    message: ChatCompletionMessage
    """A chat completion message generated by the model."""

class CompletionUsage(BaseModel):
    completion_tokens: int
    prompt_tokens: int
    total_tokens: int

class AsyncOllamaChat:
    def __init__(self, client: httpx.AsyncClient, base_url: str):
        self.client = client
        self.base_url = base_url


    # class ChatCompletionMessage(BaseModel):
    # content: Optional[str] = None
    # """The contents of the message."""

    # role: Literal["assistant"]
    # """The role of the author of this message."""

    # function_call: Optional[FunctionCall] = None
    # """Deprecated and replaced by `tool_calls`.

    # The name and arguments of a function that should be called, as generated by the
    # model.
    # """

    # tool_calls: Optional[List[ChatCompletionMessageToolCall]] = None
    # """The tool calls generated by the model, such as function calls."""


    @retry(stop=stop_after_attempt(3), wait=wait_fixed(2))
    async def generate(self, **kwargs) -> ChatCompletionMessage:
        url = f"{self.base_url}/api/generate"
        headers = {"Content-Type": "application/json"}

        prompt= ""
        messages_array = kwargs["messages"]
        model = kwargs["model"]
        for msg in messages_array:
            prompt += f'---------------{msg.get("role").value}-----------------\n'
            prompt += f'{msg.get("content")}\n'

        data = {"model": model, "prompt": prompt, "format": "json"}
        response_text = ""
        async with self.client.stream("POST", url, headers=headers, json=data) as response:
            response.raise_for_status()
            async for line in response.aiter_lines():
                if line:
                    response_json = httpx.Response(200, content=line).json()
                    response_text += response_json["response"]
                    if response_json.get("done", False):
                        break
        
        print("response_text", response_text)
        return ChatCompletionMessage(
                content=response_text,
                model=model,
                function_call=None,  # type: ignore
                tool_call=None # type: ignore
            )
    
    @retry(stop=stop_after_attempt(3), wait=wait_fixed(2))
    async def generatePro(self, model, messages_array) -> str:
        url = f"{self.base_url}/api/generate"
        headers = {"Content-Type": "application/json"}
        prompt= ""
        for msg in messages_array:
            role = msg.role
            role_value = role.value if hasattr(role, 'value') else str(role)
            prompt += f'---------------{role_value}-----------------\n'
            prompt += f'{msg.content}\n'

        data = {"model": model, "prompt": prompt, "format": "json"}
        response_text = ""
        async with self.client.stream("POST", url, headers=headers, json=data) as response:
            response.raise_for_status()
            async for line in response.aiter_lines():
                if line:
                    response_json = httpx.Response(200, content=line).json()
                    response_text += response_json["response"]
                    if response_json.get("done", False):
                        break
        
        print("response_text", response_text)
        return response_text
    
    def extract_json(text):
        # Find all occurrences of JSON-like structures
        json_matches = re.findall(r'\{(?:[^{}]|(?R))*\}', text, re.DOTALL)
        
        valid_jsons = []
        for match in json_matches:
            try:
                # Try to parse the matched string as JSON
                parsed_json = json.loads(match)
                # If successful, add to valid_jsons
                valid_jsons.append(json.dumps(parsed_json, indent=2))
            except json.JSONDecodeError:
                # If it's not valid JSON, skip it
                continue
        return '\n'.join(valid_jsons)

    @retry(stop=stop_after_attempt(99), wait=wait_fixed(2))
    async def chat(self, **kwargs) -> ChatCompletionMessage:
        url = f"{self.base_url}/api/chat"
        headers = {"Content-Type": "application/json"}

        # Convert the list of ChatMessage objects to a list of dictionaries
        messages = []
        messages_array = kwargs["messages"]
        model = kwargs["model"]
        for msg in messages_array:
            role = msg.get("role")
            role_value = role.value if hasattr(role, 'value') else str(role)
            messages.append({"role": role_value, "content": msg.get("content")})

        data = {
            "model": model,
            "messages": messages,
            "stream": True
        }

        try:
            response_text = ""
            role = ""
            async with self.client.stream("POST", url, headers=headers, json=data) as response:
                response.raise_for_status()
                async for line in response.aiter_lines():
                    if line:
                        response_json = httpx.Response(200, content=line).json()
                        print("response_json", response_json)
                        message = response_json.get('message') 
                        role = message.get('role')
                        response_text += message.get('content')
                        if response_json.get("done", False):
                            break

            response_text = response_text.replace("```json", "").replace("```", "")
            print("response_text", response_text)

            return ChatCompletion(
            id=f'random_{len(messages)}',
            choices=[{
                "message":ChatCompletionMessage(
                    content=response_text,
                    role=role,
                    function_call=None,
                    tool_call=None
                ),
                "finish_reason":'stop',
                "index":0
            }],
            model=model,
            object="chat.completion",
            created=int(time.time()),  # 정수로 변환
            usage={
                "completion_tokens":len(response_text.split()),
                "prompt_tokens":sum(len(msg.get("content", "").split()) for msg in messages),
                "total_tokens":len(response_text.split()) + sum(len(msg.get("content", "").split()) for msg in messages)
            }
        )
        except httpx.HTTPStatusError as e:
            print(f"HTTP Status Error: {e.response.status_code} - {e.response.text}")
            print(f"Response Headers: {e.response.headers}")
            print(f"Request URL: {response.url}")
            print(f"Request Body: {data}")
            raise
        except httpx.RequestError as e:
            print(f"Request Error: {e}")
            print(f"Request URL: {url}")
            print(f"Request Body: {data}")
            raise
    
class AsyncOllama:
    def __init__(self, api_key: str, base_url: str):
        self.api_key = api_key
        self.base_url = base_url
        self.client = httpx.AsyncClient()
        self.chat = AsyncOllamaChat(self.client, base_url)

    async def close(self):
        await self.client.aclose()

class OllamaSettings(ModelProviderSettings):
    credentials: Optional[OllamaCredentials]  # type: ignore
    budget: ModelProviderBudget  # type: ignore

class OllamaProvider(BaseOpenAIChatProvider[OllamaModelName, OllamaSettings]):
    CHAT_MODELS = OLLAMA_CHAT_MODELS
    MODELS = CHAT_MODELS

    default_settings = OllamaSettings(
        name="ollama_provider",
        description="Provides access to ollama API.",
        configuration=ModelProviderConfiguration(),
        credentials=None,
        budget=ModelProviderBudget(),
    )

    _settings: OllamaSettings
    _configuration: ModelProviderConfiguration
    _credentials: OllamaCredentials
    _budget: ModelProviderBudget

    def __init__(
        self,
        settings: Optional[OllamaSettings] = None,
        logger: Optional[logging.Logger] = None,
    ):
        super(OllamaProvider, self).__init__(settings=settings, logger=logger)

        self._client = AsyncOllama(
            api_key=self._credentials.api_key.get_secret_value(),
            base_url=self._credentials.api_base.get_secret_value()
        )

    async def get_available_models(
        self,
    ) -> Sequence[ChatModelInfo[OllamaModelName]]:
        return list(self.MODELS.values())    

    _ModelName = TypeVar("_ModelName", bound=str)

    # @retry(stop=stop_after_attempt(5), wait=wait_fixed(2), reraise=True)
    # async def _create_chat_completion(
    #     self,
    #     model: _ModelName,
    #     completion_kwargs: CompletionCreateParams,
    # ) -> tuple[ChatCompletion, float, int, int]:
    #     """
    #     Create a chat completion using an OpenAI-like API with retry handling

    #     Params:
    #         model: The model to use for the completion
    #         completion_kwargs: All other arguments for the completion call

    #     Returns:
    #         ChatCompletion: The chat completion response object
    #         float: The cost ($) of this completion
    #         int: Number of prompt tokens used
    #         int: Number of completion tokens used
    #     """
    #     completion_kwargs["model"] = completion_kwargs.get("model") or model

    #     print("completion_kwargs", completion_kwargs)
    #     @self._retry_api_request
    #     async def _create_chat_completion_with_retry() -> ChatCompletion:
    #         return await self._client.chat.generate(
    #             **completion_kwargs
    #         )

    #     completion = await _create_chat_completion_with_retry()

    #     if completion.usage:
    #         prompt_tokens_used = completion.usage.prompt_tokens
    #         completion_tokens_used = completion.usage.completion_tokens
    #     else:
    #         prompt_tokens_used = completion_tokens_used = 0

    #     if self._budget:
    #         cost = self._budget.update_usage_and_cost(
    #             model_info=self.CHAT_MODELS[model],
    #             input_tokens_used=prompt_tokens_used,
    #             output_tokens_used=completion_tokens_used,
    #         )
    #     else:
    #         cost = 0

    #     self._logger.debug(
    #         f"{model} completion usage: {prompt_tokens_used} input, "
    #         f"{completion_tokens_used} output - ${round(cost, 5)}"
    #     )
    #     return completion, cost, prompt_tokens_used, completion_tokens_used
    
    @retry(stop=stop_after_attempt(5), wait=wait_fixed(2), reraise=True)
    async def create_chat_completion(
        self,
        model_prompt: list[ChatMessage],
        model_name: OllamaModelName,
        completion_parser: Callable[[AssistantChatMessage], _T] = lambda _: None,
        functions: Optional[list[CompletionModelFunction]] = None,
        max_output_tokens: Optional[int] = None,
        prefill_response: str = "",
        **kwargs,
    ) -> ChatModelResponse[_T]:
        from autogpt.agents.prompt_strategies.one_shot import OneShotAgentActionProposal, OneShotAgentPromptStrategy

        (
            openai_messages,
            completion_kwargs,
            parse_kwargs,
        ) = self._get_chat_completion_args(
            prompt_messages=model_prompt,
            model=model_name,
            functions=functions,
            max_output_tokens=max_output_tokens,
            **kwargs,
        )

        print("ollama create_chat_completion",openai_messages)
        print("ollama  completion_kwargs", completion_kwargs)
        print("ollama  parse_kwargs", parse_kwargs)

        print(f"model_name: {model_name}")
        print(f"Model_prompt: {model_prompt}")
        # prompt = "\n".join([message.content for message in model_prompt])
        # print(f"message prompt: {prompt}")
        # print(f"message kwargs: {kwargs}")

        for attempt in range(2):  # 최대 3번 시도
            response_text = await self._client.chat.generatePro(model=model_name, messages_array=model_prompt)
            response_text = response_text.replace("Here is my response:", "").replace("```json", "").replace("```", "").strip()
            print(f"Response generate: {response_text}")
            # tool_calls, _errors = self._parse_assistant_tool_calls(
            #     response_text
            # )
            assistant_message = AssistantChatMessage(content=response_text)

            try:
                assistant_reply_dict = extract_dict_from_json(assistant_message.content)
                self._logger.debug(
                    "Parsing object extracted from LLM response:\n"
                    f"{json.dumps(assistant_reply_dict, indent=4)}"
                )

                # thoughts를 AssistantThoughts 인스턴스로 변환
                parsed_response = OneShotAgentActionProposal.parse_obj(assistant_reply_dict)
                print("parsed_response", parsed_response)

                # 포맷이 올바르면 루프를 빠져나옴
                break
            except (json.JSONDecodeError, ValidationError) as e:
                if attempt < 2:  # 마지막 시도가 아니면 재요청
                    print(f"Error parsing response: {e}. Requesting correct format...")
                    format_request = (
                        
                        "The response format is incorrect. Please provide the response "
                        "in the following JSON format:\n"
                        "## RESPONSE FORMAT"
                        "YOU MUST ALWAYS RESPOND WITH A JSON OBJECT OF THE FOLLOWING TYPE:"
                        "{\n"
                        "  \"thoughts\": { \n"
                        "    \"observations\": string, // Relevant observations from your last action (if any) \n"
                        "    \"text\": string, // Thoughts \n "
                        "    \"reasoning\": string, // Reasoning behind the thoughts \n"
                        "    \"self_criticism\": string, // Constructive self-criticism \n"
                        "    \"plan\": Array<string>, // Short list that conveys the long-term plan \n"
                        "    \"speak\": string // Summary of thoughts, to say to user\n"
                        "  },\n"
                        "  \"use_tool\": {\n"
                        "    \"name\": string, // open_file,open_folder,finish,read_file,write_file,list_folder,ask_user,web_search,google,read_webpage \n"
                        "    \"arguments\": Record<string, any>\n"
                        "  }\n"
                        "}\n"
                        "Please reformat your response accordingly."
                    )

                    model_prompt.append(AssistantChatMessage(content=format_request))
                else:
                    raise ValueError(f"Failed to get correct format after 3 attempts: {e}")

        parsed_result = completion_parser(assistant_message)

        response = ChatModelResponse(
            response=assistant_message,
            parsed_result=parsed_result,
            llm_info=self.CHAT_MODELS[model_name],
            prompt_tokens_used=len(model_prompt),
            completion_tokens_used=len(response_text),
            completion_parser=completion_parser,
            functions=functions,
            max_output_tokens=max_output_tokens,
            prefill_response=prefill_response,
            **kwargs
        )
        # print("ChatModelResponse", response.parsed_result)
        return response

    def get_tokenizer(self, model_name: OllamaModelName) -> ModelTokenizer[Any]:
        # HACK: No official tokenizer is available for Groq
        return tiktoken.encoding_for_model("gpt-3.5-turbo")
    